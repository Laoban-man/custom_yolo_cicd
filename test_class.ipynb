{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4daa4577-53d3-4d43-be4d-f1e1c80f2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as PilImage\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import skimage.feature as feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from utils import *\n",
    "\n",
    "\n",
    "class sealmodel:\n",
    "    def __init__(self, data_folder=\"./data/Original/\"):\n",
    "        \"\"\"\n",
    "        Initiate with folders containing the original data lists\n",
    "        \"\"\"\n",
    "        self.original_folder=data_folder+\"Train/\"\n",
    "        self.original_files = self.get_files(self.original_folder)\n",
    "        self.dotted_folder=data_folder+\"TrainDotted/\"\n",
    "        self.dotted_files = self.get_files(self.dotted_folder)\n",
    "\n",
    "    def get_files(self, data_folder):\n",
    "        \"\"\"\n",
    "        Get training file names, with and without dots\n",
    "        \"\"\"\n",
    "        folder = data_folder\n",
    "        files = [\n",
    "            os.path.join(folder, f)\n",
    "            for f in os.listdir(folder)\n",
    "            if (\n",
    "                os.path.isfile(os.path.join(folder, f))\n",
    "                and \"csv\" not in f\n",
    "                and \"json\" not in f\n",
    "            )\n",
    "        ]\n",
    "        files = sorted(files)\n",
    "        return files\n",
    "\n",
    "    def downsample_images(self,source_folder,destination_folder,dotted=False):\n",
    "        \"\"\"\n",
    "        Downsample images to make image training more manageable\n",
    "        \"\"\"\n",
    "        files = glob.glob(source_folder+\"*.jpg\")\n",
    "        i=0\n",
    "        for file in files:\n",
    "            if i % 100 == 0:\n",
    "                print(i/len(files))\n",
    "            if \"csv\" not in file and \"json\" not in file:\n",
    "                im = Image.open(file)\n",
    "                #image size\n",
    "                size=(int(im.size[0]/4),int(im.size[1]/4))\n",
    "                #resize image\n",
    "                out = im.resize(size,resample=2)\n",
    "                #save resized image\n",
    "                out.save(destination_folder+file.split(\"/\")[-1])\n",
    "                i=i+1\n",
    "        if dotted == False:\n",
    "            self.ds_original_files = self.get_files(destination_folder)\n",
    "            self.ds_original_folder=destination_folder\n",
    "        else:\n",
    "            self.ds_dotted_files = self.get_files(destination_folder)\n",
    "            self.ds__dotted_folder=destination_folder\n",
    "\n",
    "\n",
    "    def image_differences(self,destination_folder):\n",
    "        \"\"\"\n",
    "        Create a full-resolution intermediary image containing the downsampled_differences\n",
    "        between images with and without dots. Doing the same from downsampled images\n",
    "        leads to inaccurate differences resulting from pixel interpolation.\n",
    "        \"\"\"\n",
    "        folder=self.original_folder\n",
    "        files = [os.path.join(folder, f) for f in os.listdir(folder) if (os.path.isfile(os.path.join(folder, f)) and  \"csv\" not in f and \"json\" not in f)]\n",
    "        files=sorted(files)\n",
    "        folderdotted = self.dotted_folder\n",
    "        filesdotted = [os.path.join(folderdotted, f) for f in os.listdir(folderdotted) if (os.path.isfile(os.path.join(folderdotted, f)) and  \"csv\" not in f and \"json\" not in f)]\n",
    "        filesdotted=sorted(filesdotted)\n",
    "        classes = [\"adult_males\", \"subadult_males\", \"adult_females\", \"juveniles\", \"pups\", \"other\"]\n",
    "        # dataframe to store results in\n",
    "        count_df = pd.DataFrame(index=files, columns=classes).fillna(0)\n",
    "        blob_list=[]\n",
    "        for i in tqdm(range(len(files)))   :\n",
    "            # read the Train and Train Dotted images\n",
    "            image_1 = cv2.imread(files[i])\n",
    "            image_2 = cv2.imread(filesdotted[i])\n",
    "            if image_1.shape != image_2.shape:\n",
    "                image_2 = cv2.resize(image_2,(image_2.shape[1],image_2.shape[0]))\n",
    "            # absolute difference between Train and Train Dotted\n",
    "            image_3 = cv2.absdiff(image_1,image_2)\n",
    "\n",
    "            # mask out blackened regions from Train Dotted\n",
    "            mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n",
    "            mask_1[mask_1 < 20] = 0\n",
    "            mask_1[mask_1 > 0] = 255\n",
    "\n",
    "            mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "            mask_2[mask_2 < 20] = 0\n",
    "            mask_2[mask_2 > 0] = 255\n",
    "\n",
    "            image_4 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n",
    "            image_5 = cv2.bitwise_or(image_4, image_4, mask=mask_2)\n",
    "\n",
    "            # convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "            image_6 = cv2.cvtColor(image_5, cv2.COLOR_BGR2GRAY)\n",
    "            image_7= cv2.resize(image_6,(int(image_6.shape[0]/4),int(image_6.shape[1])))\n",
    "            cv2.imwrite(destination_folder+files[i].split(\"/\")[-1],image_7)\n",
    "\n",
    "\n",
    "\n",
    "    def blob_detection(self, blob_folder):\n",
    "        \"\"\"\n",
    "        Create a list containing the location and files of blobs (the differences)\n",
    "        and write them to a tensorflow-format, itself converted to a coco format\n",
    "        compatible with Detectron2 use. Downsampled images are not recommended as \n",
    "        down sampled images change the color of the dots used for seal detection.\n",
    "        \"\"\"\n",
    "        folder_blobs = blob_folder\n",
    "        files_blobs = [os.path.join(folder_blobs, f) for f in os.listdir(folder_blobs) if (os.path.isfile(os.path.join(folder_blobs, f)) and  \"csv\" not in f and \"json\" not in f)]\n",
    "        files_blobs=sorted(files_blobs)\n",
    "        classes = [\"adult_males\", \"subadult_males\", \"adult_females\", \"juveniles\", \"pups\", \"other\"]\n",
    "        # dataframe to store results in\n",
    "        count_df = pd.DataFrame(index=files_blobs, columns=classes).fillna(0)\n",
    "        blob_list=[]\n",
    "        for i in tqdm(range(len(files_blobs))):\n",
    "\n",
    "            # read the Train and Train Dotted images\n",
    "            image_1 = cv2.imread(files_blobs[i])\n",
    "\n",
    "            # detect blobs\n",
    "            blobs = feature.blob_log(image_1, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n",
    "\n",
    "            # prepare the image to plot the results on\n",
    "            #image_7 = cv2.cvtColor(image_6, cv2.COLOR_GRAY2BGR)\n",
    "            blob_type=[]\n",
    "            for blob in blobs:\n",
    "                # get the coordinates for each blob\n",
    "                y, x, s,d= blob\n",
    "                # get the color of the pixel from Train Dotted in the center of the blob\n",
    "                b,g,r = image_1[int(y)][int(x)][:]\n",
    "                # decision tree to pick the class of the blob by looking at the color in Train Dotted\n",
    "                if r > 200 and b < 50 and g < 50: # RED\n",
    "                    count_df[\"adult_males\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"adult_male\")\n",
    "                elif r > 200 and b > 200 and g < 50: # MAGENTA\n",
    "                    count_df[\"subadult_males\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"subadult_male\")\n",
    "                elif r < 100 and b < 100 and 150 < g < 200: # GREEN\n",
    "                    count_df[\"pups\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"pup\")\n",
    "                elif r < 100 and  100 < b and g < 100: # BLUE\n",
    "                    count_df[\"juveniles\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"juvenile\")\n",
    "                elif r < 150 and b < 50 and g < 100:  # BROWN\n",
    "                    count_df[\"adult_females\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"adult_female\")\n",
    "                else:\n",
    "                    count_df[\"other\"][files_blobs[i]] += 1\n",
    "                    blob_type.append(\"other\")\n",
    "            blob_list.append([blobs,blob_type,files_blobs[i]])\n",
    "\n",
    "        img_bb_list=[]\n",
    "        for blobs in blob_list:\n",
    "            bb_list=[]\n",
    "            i=0\n",
    "            for blob in blobs[0]:\n",
    "                y,x,s,d=blob\n",
    "                if blobs[1][i]==\"adult_male\":\n",
    "                    size=15\n",
    "                elif blobs[1][i]==\"subadult_male\":\n",
    "                    size=13\n",
    "                elif blobs[1][i]==\"pup\":\n",
    "                    size=5\n",
    "                elif blobs[1][i]==\"juvenile\":\n",
    "                    size=10\n",
    "                elif blobs[1][i]==\"adult_female\":\n",
    "                    size=10\n",
    "                elif blobs[1][i]==\"other\":\n",
    "                    size=10\n",
    "                y_min=np.int(np.floor(y-(size)*s))\n",
    "                y_max=np.int(np.floor(y+(size)*s))\n",
    "                x_min=np.int(np.floor(x-(size)*s))\n",
    "                x_max=np.int(np.floor(x+(size)*s))\n",
    "                bb_list.append([y_min,y_max,x_min,x_max])\n",
    "                i=i+1\n",
    "            img_bb_list.append(bb_list)\n",
    "        self.blobs=img_bb_list\n",
    "        self.c2tensorflow(blob_list,img_bb_list)\n",
    "\n",
    "    def c2tensorflow(self,blob_list,img_bb_list,destination_folder=\"\"):\n",
    "        \"\"\"\n",
    "        Convert blob list with annotations to a tensorflow format\n",
    "        \"\"\"\n",
    "        print(\"Convert to TS format\")\n",
    "        temp=[]\n",
    "        for i in range(len(blob_list)):\n",
    "            img_shape=plt.imread(self.ds_original_files[i]).shape\n",
    "            for j in range(len(img_bb_list[i])):\n",
    "                temp.append(np.array([self.ds_original_files[i],blob_list[i][1][j],img_shape[1],img_shape[0],img_bb_list[i][j][2],img_bb_list[i][j][0], img_bb_list[i][j][3], img_bb_list[i][j][1]]))\n",
    "\n",
    "        file_data=pd.DataFrame(temp,columns=[\"filename\",\"class\",\"width\",\"height\",\"xmin\",\"ymin\",\"xmax\",\"ymax\"])\n",
    "        file_data.to_csv(destination_folder+\"tensorflow.csv\")\n",
    "        self.c2coco(destination_folder+\"tensorflow.csv\")\n",
    "\n",
    "    def c2coco(self,file_path):\n",
    "        \"\"\"\n",
    "        Convert tensorflow annotation format to coco format\n",
    "        \"\"\"\n",
    "        print(\"Convert to coco format\")\n",
    "        save_json_path = 'annotations.json'\n",
    "        data = pd.read_csv(\"tensorflow.csv\")\n",
    "\n",
    "        images = []\n",
    "        categories = []\n",
    "        annotations = []\n",
    "\n",
    "        category = {}\n",
    "        category[\"supercategory\"] = 'None'\n",
    "        category[\"id\"] = 0\n",
    "        category[\"name\"] = 'None'\n",
    "        categories.append(category)\n",
    "\n",
    "        data['fileid'] = data['filename'].astype('category').cat.codes\n",
    "        data['categoryid']= pd.Categorical(data['class'],ordered= True).codes\n",
    "        data['categoryid'] = data['categoryid']+1\n",
    "        data['annid'] = data.index\n",
    "        \n",
    "        def image(row):\n",
    "            image = {}\n",
    "            image[\"height\"] = row.height\n",
    "            image[\"width\"] = row.width\n",
    "            image[\"id\"] = row.fileid\n",
    "            image[\"file_name\"] = row.filename\n",
    "            return image\n",
    "\n",
    "        def category(row):\n",
    "            category = {}\n",
    "            category[\"supercategory\"] = 'Seal'\n",
    "            category[\"id\"] = row.categoryid\n",
    "            category[\"name\"] = row[3]\n",
    "            return category\n",
    "\n",
    "        def annotation(row):\n",
    "            annotation = {}\n",
    "            area = (row.xmax -row.xmin)*(row.ymax - row.ymin)\n",
    "            annotation[\"segmentation\"] = []\n",
    "            annotation[\"iscrowd\"] = 0\n",
    "            annotation[\"area\"] = area\n",
    "            annotation[\"image_id\"] = row.fileid\n",
    "            annotation[\"bbox\"] = [row.xmin, row.ymin, row.xmax -row.xmin,row.ymax-row.ymin ]\n",
    "            annotation[\"category_id\"] = row.categoryid\n",
    "            annotation[\"id\"] = row.annid\n",
    "            return annotation\n",
    "\n",
    "        for row in data.itertuples():\n",
    "            annotations.append(annotation(row))\n",
    "\n",
    "        imagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\n",
    "        for row in imagedf.itertuples():\n",
    "            images.append(image(row))\n",
    "\n",
    "        catdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\n",
    "        for row in catdf.itertuples():\n",
    "            row\n",
    "            categories.append(category(row))\n",
    "\n",
    "        data_coco = {}\n",
    "        data_coco[\"images\"] = images\n",
    "        data_coco[\"categories\"] = categories\n",
    "        data_coco[\"annotations\"] = annotations\n",
    "        data_coco[\"info\"] = {}\n",
    "        data_coco[\"licenses\"] = {}\n",
    "        json.dump(data_coco, open(save_json_path, \"w\"), indent=4)\n",
    "        self.coco2yolo(output_path=\"./data/Downsampled/Train/\",file_path=\"annotations.json\")\n",
    "        \n",
    "    def coco2yolo(self,output_path,file_path):\n",
    "        \"\"\"\n",
    "        Convert the yolo annotations to coco\n",
    "        \"\"\"\n",
    "        with open(file_path) as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # write _darknet.labels, which holds names of all classes (one class per line)\n",
    "        label_file = os.path.join(output_path, \"_darknet.labels\")\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for category in tqdm(json_data[\"categories\"], desc=\"Categories\"):\n",
    "                category_name = category[\"name\"]\n",
    "                f.write(f\"{category_name}\\n\")\n",
    "\n",
    "        for image in tqdm(json_data[\"images\"], desc=\"Annotation txt for each image\"):\n",
    "            img_id = image[\"id\"]\n",
    "            img_name = image[\"file_name\"]\n",
    "            img_width = image[\"width\"]\n",
    "            img_height = image[\"height\"]\n",
    "\n",
    "            anno_in_image = [anno for anno in json_data[\"annotations\"] if anno[\"image_id\"] == img_id]\n",
    "            anno_txt = os.path.join(output_path, img_name.split(\"/\")[-1].split(\".\")[0] + \".txt\")\n",
    "            with open(anno_txt, \"w\") as f:\n",
    "                for anno in anno_in_image:\n",
    "                    category = anno[\"category_id\"]\n",
    "                    bbox_COCO = anno[\"bbox\"]\n",
    "                    x, y, w, h = convert_bbox_coco2yolo(img_width, img_height, bbox_COCO)\n",
    "                    f.write(f\"{category} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "    def predict(self,image_path=\"933.jpg\"):\n",
    "        \"\"\"\n",
    "        Initiate a Detectron2 model, returns instances detected with the boudning boxes\n",
    "        and the category associated.\n",
    "        \"\"\"\n",
    "        # Model\n",
    "        model = torch.hub.load('yolov5','custom', path='best.pt',force_reload=True,source='local')\n",
    "\n",
    "        # Images\n",
    "        img = image_path  \n",
    "\n",
    "        # Inference\n",
    "        results = model(img)\n",
    "\n",
    "        # Results\n",
    "        results.print()\n",
    "        img=results.imgs[0]\n",
    "        im = Image.fromarray(img)\n",
    "        im.save(\"result.jpg\")\n",
    "        return results\n",
    "    \n",
    "    def move_training_images(self):\n",
    "        \"\"\"\n",
    "        Move images into folder tree expected by yolov5\n",
    "        \"\"\"\n",
    "        images = [os.path.join('./data/Downsampled/Train/', x) for x in os.listdir('./data/Downsampled/Train') if x[-3:]==\"jpg\"]\n",
    "        annotations = [os.path.join('./data/Downsampled/Train/', x) for x in os.listdir('./data/Downsampled/Train/') if x[-3:] == \"txt\"]\n",
    "\n",
    "        images.sort()\n",
    "        annotations.sort()\n",
    "\n",
    "        # Split the dataset into train-valid-test splits \n",
    "        train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
    "        val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)\n",
    "        \n",
    "        path = './datasets/images/train'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        path = './datasets/images/val'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        path = './datasets/images/test'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        path = './datasets/labels/train'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        path = './datasets/labels/test'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        path = './datasets/labels/val'\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Move the splits into their folders\n",
    "        copy_files_to_folder(train_images, './datasets/images/train')\n",
    "        copy_files_to_folder(val_images, './datasets/images/val/')\n",
    "        copy_files_to_folder(test_images, './datasets/images/test/')\n",
    "        copy_files_to_folder(train_annotations, './datasets/labels/train/')\n",
    "        copy_files_to_folder(val_annotations, './datasets/labels/val/')\n",
    "        copy_files_to_folder(test_annotations, './datasets/labels/test/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99b81ae8-4da0-42e9-9844-3e1cdbecd98c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n",
      "100%|██████████| 10/10 [02:17<00:00, 13.71s/it]\n",
      "/home/hysterio/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/ipykernel_launcher.py:187: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/hysterio/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/ipykernel_launcher.py:188: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/hysterio/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/ipykernel_launcher.py:189: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/hysterio/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/ipykernel_launcher.py:190: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to TS format\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file './data/Downsampled/Train/0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-5af5439a2de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Original/TrainDotted/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data/Downsampled/TrainDotted/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdotted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_differences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Downsampled/blobs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Downsampled/blobs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco2yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/Downsampled/Train/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"annotations.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_training_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-29fc47ad0a03>\u001b[0m in \u001b[0;36mblob_detection\u001b[0;34m(self, blob_folder)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mimg_bb_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc2tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mc2tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblob_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-29fc47ad0a03>\u001b[0m in \u001b[0;36mc2tensorflow\u001b[0;34m(self, blob_list, img_bb_list, destination_folder)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mimg_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_original_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_original_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblob_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_bb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_bb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_bb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1562\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/vivadata/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 3148\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m     )\n\u001b[1;32m   3150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file './data/Downsampled/Train/0.txt'"
     ]
    }
   ],
   "source": [
    "test=sealmodel()\n",
    "test.downsample_images(\"./data/Original/Train/\",\"./data/Downsampled/Train/\")\n",
    "test.downsample_images(\"./data/Original/TrainDotted/\",\"./data/Downsampled/TrainDotted/\",dotted=True)\n",
    "test.image_differences(\"./data/Downsampled/blobs/\")\n",
    "test.blob_detection(\"./data/Downsampled/blobs/\")\n",
    "test.coco2yolo(\"./data/Downsampled/Train/\",\"annotations.json\")\n",
    "test.move_training_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380d22d5-c7fe-4515-80b3-6da31d9df7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.1-394-gd7bc5d7 Python-3.7.6 torch-1.12.1+cu102 CUDA:0 (NVIDIA GeForce GTX 1660 Ti with Max-Q Design, 5945MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 280 layers, 12331312 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 331x516 31 adult_females, 2 adult_males\n",
      "Speed: 4.1ms pre-process, 10.2ms inference, 10.6ms NMS per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "test=sealmodel(\"./code/data/Downsampled/\")\n",
    "result=test.predict(\"./test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e5d7d449-4323-4709-8519-03bb452e5ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[9.97467e+02, 4.13337e+02, 1.03010e+03, 4.45946e+02, 4.55757e-01, 1.00000e+00],\n",
       "         [1.72264e+02, 2.63389e+02, 2.05205e+02, 2.96963e+02, 3.96704e-01, 1.00000e+00],\n",
       "         [7.43280e+02, 6.38882e+02, 7.77289e+02, 6.72922e+02, 3.61869e-01, 1.00000e+00],\n",
       "         [5.41036e+02, 7.95377e+02, 5.74740e+02, 8.29353e+02, 3.41705e-01, 1.00000e+00],\n",
       "         [9.93950e+02, 5.25263e+02, 1.02783e+03, 5.59566e+02, 3.01699e-01, 1.00000e+00],\n",
       "         [2.01204e+02, 2.33862e+02, 2.35515e+02, 2.68292e+02, 2.97890e-01, 1.00000e+00],\n",
       "         [7.95064e+02, 7.08040e+02, 8.31158e+02, 7.44107e+02, 2.96807e-01, 1.00000e+00],\n",
       "         [7.42307e+02, 5.24691e+02, 7.79464e+02, 5.60158e+02, 2.85562e-01, 1.00000e+00],\n",
       "         [9.48672e+02, 8.72644e+02, 9.82383e+02, 9.07189e+02, 2.80435e-01, 1.00000e+00],\n",
       "         [7.10334e+02, 5.50245e+02, 7.46548e+02, 5.85795e+02, 2.66230e-01, 1.00000e+00],\n",
       "         [9.97136e+02, 3.75598e+02, 1.03093e+03, 4.09260e+02, 2.62785e-01, 1.00000e+00]], device='cuda:0')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(result.img[0])\n",
    "for a in result.xyxy:\n",
    "    plt.rect(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "258a3f6d-b266-4d91-b692-02e0a3af9b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image=result.render()\n",
    "im = Image.fromarray(image[0])\n",
    "im.save(\"result.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
